{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7347482a-142a-420e-b2ff-1f42191162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6492ee-2a0d-4f1e-a45c-0d7686ead888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b86525e-de6b-470a-8981-79a6dba4f2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset in pandas\n",
    "\n",
    "df = pd.read_csv('data/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfd8b11f-b271-42e0-a085-96670d1bd2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average review length: 231\n"
     ]
    }
   ],
   "source": [
    "avg_len = int(df['review'].apply(lambda x: len(x.split())).mean())\n",
    "print(\"Average review length:\", avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d96846b-e077-4aa2-9a0c-200ace036db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile review length: 590\n"
     ]
    }
   ],
   "source": [
    "lengths = df['review'].apply(lambda x: len(x.split()))\n",
    "max_len_95 = int(np.percentile(lengths, 95))\n",
    "print(\"95th percentile review length:\", max_len_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6de6362-f0a0-4139-bddd-b258fe5b7604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41353da9-2953-4387-bffb-4bdffea80bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96999c20-9aa9-4fc5-86a2-c9219d3ed1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48dc88c6-54d8-4bd1-8cce-9b91f6a4bbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e934da1-5b66-441d-8c10-4ef1fcffd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Lower text just in case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # str.maketrans is much faster\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra space\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "920305de-e34c-4c2d-896a-0adfc98196ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e3115b9-efa9-45cd-8b85-46c004b879dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production the filming technique is very unassuming very oldtimebbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great masters of comedy and his life the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(df['review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00d13f7a-0aed-4595-8b9c-7a74c733d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize and split the words by lowering them and splitting them by ' '\n",
    "def tokenize_line(line):\n",
    "    words = line.lower().split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "324c8799-9a48-4eb5-8918-a810c5788e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "line = 'I love pizza'\n",
    "print(tokenize_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48500adb-143f-4d74-a0ab-8c589f749b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocab(tokenized_texts, min_freq = 2):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_texts:\n",
    "        counter.update(tokens)\n",
    "        # print(counter)\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.items():\n",
    "        # print(f\"{word}: {freq}\")\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "            # print(f\"Final Vocab: {vocab}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e3e16de-f2ea-4787-a5bf-a31a36d16ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<unk>': 1, 'i': 2, 'love': 3, 'this': 4, 'movie': 5, 'hate': 6}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "texts = [\"I love this movie\", \"This movie is hate\", \"Love it\", \"I hate this movie\"]\n",
    "tokenized_texts = [tokenize_line(t) for t in texts]\n",
    "vocab = build_vocab(tokenized_texts)\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f121a4a7-4b50-47a2-886a-76ba4ee87632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0, '<unk>': 1, 'i': 2, 'love': 3, 'this': 4, 'movie': 5, 'hate': 6}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8664b81-b282-4f55-96be-4244dc00c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode text to int  \n",
    "def encode_text(tokenized_texts, vocab):\n",
    "    encoded_texts = []\n",
    "    for tokens in tokenized_texts:\n",
    "        encoded = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "        # print(f\"Encoded: {encoded}\")\n",
    "        encoded_texts.append(encoded)\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1f985e7-fe24-4c25-8200-5ca50ed296ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after encoding: [[2, 3, 4, 5], [4, 5, 1, 6], [3, 1], [2, 6, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "encoded_texts = encode_text(tokenized_texts=tokenized_texts, vocab= vocab)\n",
    "print(f\"Text after encoding: {encoded_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbd00fc5-4d69-4e1b-9dfe-9e8dbfba622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding for shorter sentences\n",
    "def pad_sequences(encoded_text, max_len):\n",
    "    padded_texts = []\n",
    "    for seq in encoded_text:\n",
    "        if len(seq) < max_len:\n",
    "            # Pad with 0's (for <pad)\n",
    "            seq = seq + [0] * (max_len - len(seq))\n",
    "        else:\n",
    "            # Truncate if too long\n",
    "            seq = seq[:max_len]\n",
    "        padded_texts.append(seq)\n",
    "    return padded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "931fc08a-708b-49a8-973a-b823153e9a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [4, 5, 1, 6], [3, 1, 0, 0], [2, 6, 4, 5]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "max_len = max(len(seq) for seq in encoded_texts) \n",
    "pad_sequences(encoded_texts, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4b4856e-2685-4cac-bbf9-f9ae556bad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vocab(texts, min_freq = 2):\n",
    "    cleaned = texts.apply(clean_text)\n",
    "    tokenized = cleaned.apply(tokenize_line).tolist()\n",
    "    vocab = build_vocab(tokenized, min_freq)\n",
    "    return vocab, tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ea613de-99a2-4ff3-a3ed-3220d85ba0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(tokenized_texts, vocab, max_len = 100):\n",
    "    encoded = encode_text(tokenized_texts, vocab)\n",
    "    padded = pad_sequences(encoded, max_len)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d625baca-a43f-4c09-bb5b-b91b4588ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "def convert_to_tensors(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype = torch.long)\n",
    "    y_tensor = torch.tensor(y, dtype = torch.long)\n",
    "    return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20df84c8-bfae-4166-914d-07038122fafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = (df['sentiment'].str.lower() == 'positive').astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f96583c5-1fde-42b1-b6e6-0fa3fe92b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all functions into one class\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df, max_len = 100, min_freq = 10, build_vocab = True, vocab = None):\n",
    "        \"\"\"\n",
    "        df: pandas DataFrame with columns 'review' and 'sentiment'\n",
    "        max_len: max sequence length for padding\n",
    "        min_freq: minimum frequency to keep a word in vocab\n",
    "        build_vocab: True if building vocab from df (train), False for test/new data\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.min_freq = min_freq\n",
    "        self.vocab = None\n",
    "        # Encode sentiment to binary labels\n",
    "        self.labels = df['sentiment'].values\n",
    "        # Clean and tokenize labels\n",
    "        self.texts = df['review'].apply(self.clean_text).apply(self.tokenize_line).tolist()\n",
    "\n",
    "        # Build vocab if required for training\n",
    "        if build_vocab:\n",
    "            self.vocab = self.build_vocab(self.texts, self.min_freq)\n",
    "        else:\n",
    "            if vocab is None:\n",
    "                raise ValueError(\"Vocab must be provided if build_vocab is False\")\n",
    "            self.vocab = vocab\n",
    "        self.encoded_texts = self.encode_and_pad(self.texts, self.vocab, self.max_len)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        # Return encoded tensor and label tensor\n",
    "        return torch.tensor(self.encoded_texts[idx], dtype = torch.long), torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        # Lower text just in case\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # str.maketrans is much faster\n",
    "        # Remove digits\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Remove extra space\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    @staticmethod\n",
    "    def tokenize_line(line):\n",
    "        words = line.lower().split(' ')\n",
    "        return words \n",
    "    # Build vocabulary\n",
    "    @staticmethod\n",
    "    def build_vocab(tokenized_texts, min_freq = 2):\n",
    "        counter = Counter()\n",
    "        for tokens in tokenized_texts:\n",
    "            counter.update(tokens)\n",
    "            # print(counter)\n",
    "        vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        for word, freq in counter.items():\n",
    "            # print(f\"{word}: {freq}\")\n",
    "            if freq >= min_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "                # print(f\"Final Vocab: {vocab}\")\n",
    "        return vocab\n",
    "    def encode_and_pad(self, tokenized_texts, vocab, max_len):\n",
    "        encoded = []\n",
    "        for tokens in tokenized_texts:\n",
    "            enc = [vocab.get(token, 1) for token in tokens]\n",
    "            # pad or truncate\n",
    "            if len(enc) < max_len:\n",
    "                enc.extend([0] * (max_len - len(enc)))\n",
    "            else:\n",
    "                enc = enc[:max_len]\n",
    "            encoded.append(enc)\n",
    "        return encoded\n",
    "    def encode_text(self, text):\n",
    "        # Clean, tokenize, encode, and a pad a single string (for new data)\n",
    "        clean = self.clean_text(text)\n",
    "        tokens = self.tokenize_line(clean)\n",
    "        enc = [self.vocab.get(token, 1) for token in tokens]\n",
    "        if len(enc) < self.max_len:\n",
    "            enc.extend([0] * (self.max_len - len(enc)))\n",
    "        else:\n",
    "            enc = enc[:self.max_len]\n",
    "        return torch.tensor(enc, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66a7ee81-1f8f-4c1c-be9c-5354025287b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size = 0.2, stratify = df['sentiment'], random_state = 42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['sentiment'], random_state=42)\n",
    "\n",
    "# Create train_dataset\n",
    "train_dataset = IMDBDataset(train_df, build_vocab=True, max_len=250)\n",
    "val_dataset = IMDBDataset(val_df, build_vocab=False, vocab=train_dataset.vocab, max_len=250)\n",
    "test_dataset = IMDBDataset(test_df, build_vocab=False, vocab=train_dataset.vocab, max_len=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b888f561-01c4-4b15-b749-594c7d5e486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataloader to load data into model\n",
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bc4ba3a-b1fc-45b8-b91d-5d62b4fae094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2c61f93-a78c-4791-912e-9d8a49a0d4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 250]), torch.Size([32]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e39c9dc0-8d27-4b9d-b22f-74a55dae5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e128d1f-6c41-4791-927c-af4eb3609de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Embedding layer: converts vocab to dense vectors of size embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx= 0)\n",
    "        # RNN layer: processes sequences of embeddings, outputs hidden states of size hidden_dim\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        # Fully connected layer: maps the final hidden state to output_dim (e.g. number of classes)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input indices through embedding layer -> shape: (batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass embeddings through RNN -> output (all hidden states), hidden (last hidden state)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # Use last hidden state for classification; squeeze removes the extra dimension -> shape: (batch_size, hidden_dim)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        # Return the logits (unnormalized scores) for each class\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ba346f8-9f77-4b3c-a78a-e2ec34b2299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = SentimentRNN(\n",
    "    vocab_size= len(train_dataset.vocab),\n",
    "    embed_dim = 64, \n",
    "    hidden_dim= 128,\n",
    "    output_dim=1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca9eda86-1cf7-40d8-aba3-4560fcda8530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.weight',\n",
       "              tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.3466, -0.1973, -1.0546,  ...,  0.5069, -0.4752, -0.4920],\n",
       "                      [-0.1360,  1.6354,  0.6547,  ..., -0.7251,  0.4664,  0.6667],\n",
       "                      ...,\n",
       "                      [ 0.7116, -0.7279, -0.3306,  ...,  1.9188,  0.6521,  1.6782],\n",
       "                      [-1.0569, -0.1974, -1.5052,  ...,  0.3588, -1.0410,  0.0264],\n",
       "                      [ 2.4404,  0.7952, -1.9100,  ..., -0.1307,  0.6438, -0.4163]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_ih_l0',\n",
       "              tensor([[ 0.0782,  0.0164,  0.0417,  ..., -0.0743, -0.0743, -0.0678],\n",
       "                      [ 0.0316, -0.0161, -0.0125,  ..., -0.0736, -0.0491,  0.0744],\n",
       "                      [-0.0772,  0.0281,  0.0540,  ...,  0.0837, -0.0537, -0.0848],\n",
       "                      ...,\n",
       "                      [-0.0734,  0.0384, -0.0134,  ..., -0.0033,  0.0793,  0.0516],\n",
       "                      [-0.0452,  0.0758, -0.0454,  ...,  0.0187, -0.0196,  0.0534],\n",
       "                      [ 0.0497,  0.0662,  0.0577,  ...,  0.0253, -0.0779,  0.0121]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.weight_hh_l0',\n",
       "              tensor([[-0.0740, -0.0652,  0.0195,  ...,  0.0773, -0.0586, -0.0091],\n",
       "                      [-0.0652, -0.0328, -0.0174,  ..., -0.0295,  0.0750,  0.0009],\n",
       "                      [-0.0090, -0.0121, -0.0299,  ...,  0.0448, -0.0705,  0.0072],\n",
       "                      ...,\n",
       "                      [ 0.0808, -0.0288, -0.0225,  ...,  0.0325,  0.0445,  0.0032],\n",
       "                      [-0.0285,  0.0816, -0.0669,  ...,  0.0132,  0.0054, -0.0151],\n",
       "                      [ 0.0200,  0.0710,  0.0546,  ..., -0.0576, -0.0450, -0.0828]],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_ih_l0',\n",
       "              tensor([-0.0182, -0.0858,  0.0015,  0.0848, -0.0617,  0.0402, -0.0443, -0.0751,\n",
       "                      -0.0674,  0.0453, -0.0529, -0.0436,  0.0087,  0.0847,  0.0018, -0.0783,\n",
       "                       0.0498, -0.0482,  0.0809, -0.0411, -0.0522,  0.0523,  0.0862, -0.0654,\n",
       "                       0.0032, -0.0439,  0.0658,  0.0609, -0.0878, -0.0368,  0.0064,  0.0677,\n",
       "                       0.0220, -0.0292,  0.0642, -0.0368,  0.0510,  0.0795, -0.0623,  0.0410,\n",
       "                      -0.0259, -0.0742, -0.0123, -0.0092, -0.0725, -0.0695, -0.0159,  0.0492,\n",
       "                      -0.0166,  0.0194, -0.0638, -0.0307, -0.0783, -0.0038, -0.0304,  0.0450,\n",
       "                       0.0238,  0.0133,  0.0153,  0.0243,  0.0010, -0.0010, -0.0348, -0.0644,\n",
       "                       0.0358,  0.0821,  0.0361, -0.0722, -0.0045,  0.0454,  0.0116, -0.0239,\n",
       "                      -0.0598,  0.0215,  0.0021, -0.0667, -0.0169,  0.0111, -0.0837, -0.0182,\n",
       "                      -0.0466,  0.0314,  0.0745,  0.0647,  0.0208, -0.0615, -0.0013, -0.0308,\n",
       "                       0.0504, -0.0575,  0.0711,  0.0357,  0.0136,  0.0032, -0.0399,  0.0772,\n",
       "                       0.0603,  0.0485,  0.0442,  0.0621, -0.0826, -0.0313,  0.0083,  0.0138,\n",
       "                       0.0362, -0.0085, -0.0303, -0.0851, -0.0754, -0.0078, -0.0667,  0.0227,\n",
       "                       0.0421,  0.0520,  0.0646, -0.0433,  0.0101, -0.0482,  0.0399,  0.0639,\n",
       "                       0.0811, -0.0342, -0.0057,  0.0056, -0.0858, -0.0517, -0.0192,  0.0769],\n",
       "                     device='cuda:0')),\n",
       "             ('rnn.bias_hh_l0',\n",
       "              tensor([ 0.0347, -0.0258, -0.0433,  0.0259,  0.0253, -0.0220,  0.0882,  0.0188,\n",
       "                      -0.0290,  0.0688, -0.0397, -0.0419,  0.0380,  0.0473, -0.0771, -0.0844,\n",
       "                      -0.0322,  0.0299, -0.0110, -0.0162, -0.0399, -0.0711, -0.0287,  0.0600,\n",
       "                       0.0311, -0.0033, -0.0164, -0.0496, -0.0037, -0.0102,  0.0509, -0.0099,\n",
       "                      -0.0043, -0.0857,  0.0200,  0.0183,  0.0036,  0.0507, -0.0200, -0.0271,\n",
       "                      -0.0217,  0.0018,  0.0445, -0.0435,  0.0804, -0.0181, -0.0249,  0.0286,\n",
       "                      -0.0384, -0.0727,  0.0069,  0.0717, -0.0508, -0.0373,  0.0495, -0.0600,\n",
       "                      -0.0215, -0.0368, -0.0772, -0.0223,  0.0366,  0.0430, -0.0537, -0.0047,\n",
       "                       0.0319, -0.0669, -0.0263,  0.0693,  0.0849, -0.0221,  0.0113,  0.0061,\n",
       "                      -0.0204, -0.0462, -0.0118,  0.0741, -0.0778,  0.0498,  0.0838,  0.0205,\n",
       "                      -0.0141,  0.0495,  0.0876, -0.0449, -0.0632, -0.0640,  0.0457,  0.0679,\n",
       "                       0.0203, -0.0206, -0.0266,  0.0625, -0.0793,  0.0630, -0.0755,  0.0263,\n",
       "                       0.0712,  0.0509, -0.0427, -0.0476,  0.0782, -0.0552,  0.0755, -0.0047,\n",
       "                      -0.0126,  0.0755,  0.0411, -0.0131,  0.0726,  0.0837,  0.0396,  0.0636,\n",
       "                      -0.0245, -0.0760,  0.0445,  0.0151,  0.0275, -0.0115,  0.0185, -0.0568,\n",
       "                       0.0442,  0.0018, -0.0618, -0.0290, -0.0639,  0.0854, -0.0495, -0.0155],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.weight',\n",
       "              tensor([[ 0.0876, -0.0615, -0.0080, -0.0145, -0.0868, -0.0352, -0.0002, -0.0137,\n",
       "                        0.0145, -0.0611,  0.0846,  0.0563,  0.0825,  0.0874,  0.0243, -0.0852,\n",
       "                        0.0142, -0.0753,  0.0122,  0.0585, -0.0060, -0.0699,  0.0856, -0.0611,\n",
       "                       -0.0052, -0.0641, -0.0274,  0.0277, -0.0228,  0.0251, -0.0498,  0.0427,\n",
       "                       -0.0778, -0.0271,  0.0329, -0.0845,  0.0851, -0.0747, -0.0363, -0.0222,\n",
       "                       -0.0260, -0.0810, -0.0636, -0.0394, -0.0599, -0.0496,  0.0126,  0.0387,\n",
       "                       -0.0411, -0.0279,  0.0163,  0.0883, -0.0587, -0.0068, -0.0803, -0.0265,\n",
       "                       -0.0099, -0.0163,  0.0265,  0.0138, -0.0776, -0.0459, -0.0835, -0.0662,\n",
       "                       -0.0199, -0.0159,  0.0736,  0.0742,  0.0800,  0.0256,  0.0281, -0.0696,\n",
       "                       -0.0447,  0.0006,  0.0138, -0.0142, -0.0066,  0.0368, -0.0154, -0.0370,\n",
       "                       -0.0717,  0.0538,  0.0148, -0.0484,  0.0419, -0.0275, -0.0758, -0.0229,\n",
       "                       -0.0067, -0.0294, -0.0008, -0.0140, -0.0765, -0.0871, -0.0751, -0.0834,\n",
       "                        0.0194,  0.0068, -0.0111,  0.0442, -0.0283,  0.0615, -0.0359, -0.0201,\n",
       "                        0.0724, -0.0343,  0.0451, -0.0589,  0.0663, -0.0236,  0.0504,  0.0114,\n",
       "                       -0.0874, -0.0189,  0.0615,  0.0270, -0.0724, -0.0539, -0.0875,  0.0777,\n",
       "                        0.0684, -0.0578,  0.0714, -0.0862, -0.0637, -0.0858, -0.0144,  0.0041]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc.bias', tensor([-0.0764], device='cuda:0'))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "48674572-8720-485e-946c-8be651e427ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_v1.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7be09ba-97c4-4e48-8b8e-d6c3f6692e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(preds, labels):\n",
    "    return (preds == labels).sum().item() / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0813afd-5910-4fe8-9f15-06172530edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model:nn.Module,\n",
    "                train_dataloader: torch.utils.data,\n",
    "                optimizer: torch.optim,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                device: torch.device= device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0,0\n",
    "    for X, y in train_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Reset optimizer to 0 \n",
    "        optimizer.zero_grad()\n",
    "        # Make preds\n",
    "        y_pred = model(X).squeeze(1)\n",
    "        # Calcualte loss\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0) # Multiply just in case last batch size is less than 32\n",
    "        preds = (torch.sigmoid(y_pred) >= 0.5).float()\n",
    "        train_acc += accuracy_fn(preds, y) * X.size(0)\n",
    "    avg_loss = train_loss / len(train_dataloader.dataset)\n",
    "    avg_acc = train_acc / len(train_dataloader.dataset)\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Train Accuracy: {avg_acc:.4f}\")\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5decf31d-6f67-4272-afb1-d84c62874b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_step(model: nn.Module,\n",
    "                  test_dataloader: torch.utils.data,\n",
    "                  loss_fn: torch.nn.Module,\n",
    "                  device: torch.device = device):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    total_samples = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            test_pred = model(X_test).squeeze(1)\n",
    "            # Pass raw logits to loss_fn\n",
    "            test_loss += loss_fn(test_pred, y_test.float()).item() * X_test.size(0)\n",
    "            preds = (torch.sigmoid(test_pred) >= 0.5).float()\n",
    "            test_acc += (preds == y_test).sum().item()\n",
    "            total_samples += y_test.size(0)\n",
    "    avg_loss = test_loss / total_samples\n",
    "    avg_acc = test_acc / total_samples\n",
    "    print(f\"Test Loss: {avg_loss:.4f} |  Test Accuracy: {avg_acc:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b46b7bb7-d7fa-4fc2-b8b9-4888d053a1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f676781f0e24cb88a791efbcc7d7b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0---\n",
      "\n",
      "Train Loss: 0.6956 | Train Accuracy: 0.4975\n",
      "Test Loss: 0.6933 |  Test Accuracy: 0.5008\n",
      "Epoch: 1---\n",
      "\n",
      "Train Loss: 0.6959 | Train Accuracy: 0.5079\n",
      "Test Loss: 0.6949 |  Test Accuracy: 0.5022\n",
      "Epoch: 2---\n",
      "\n",
      "Train Loss: 0.6962 | Train Accuracy: 0.5003\n",
      "Test Loss: 0.6965 |  Test Accuracy: 0.4978\n",
      "Epoch: 3---\n",
      "\n",
      "Train Loss: 0.6957 | Train Accuracy: 0.5024\n",
      "Test Loss: 0.6938 |  Test Accuracy: 0.5025\n",
      "Epoch: 4---\n",
      "\n",
      "Train Loss: 0.6941 | Train Accuracy: 0.5118\n",
      "Test Loss: 0.6939 |  Test Accuracy: 0.5038\n",
      "Train time on cuda:0: 42.125 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "torch.manual_seed(42)\n",
    "train_time_start_gpu= timer()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}---\\n\")\n",
    "    train_step(\n",
    "        model= model_v1,\n",
    "        train_dataloader=train_loader,\n",
    "        optimizer = optimizer, \n",
    "        loss_fn=loss_fn, \n",
    "        )\n",
    "    validate_step(model= model_v1,\n",
    "        test_dataloader=val_loader,\n",
    "        loss_fn=loss_fn, \n",
    "        )\n",
    "train_time_end_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(\n",
    "    start = train_time_start_gpu,\n",
    "    end = train_time_end_gpu,\n",
    "    device=str(next(model_v1.parameters()).device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b6edf2c-4e91-44d2-b43a-01f8be32fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6582b690-1765-4259-9f21-6131a837a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Embedding layer: converts vocab to dense vectors of size embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = 0)\n",
    "        # RNN layer: processes sequences of embeddings, outputs hidden states of size hidden_dim\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)  # Add dropout layer\n",
    "\n",
    "        # Fully connected layer: maps the final hidden state to output_dim (e.g. number of classes)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        # Pass input indices through embedding layer -> shape: (batch_size, seq_len, embed_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        # Pass embeddings through RNN -> output (all hidden states), hidden (last hidden state)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # Use last hidden state for classification; squeeze removes the extra dimension -> shape: (batch_size, hidden_dim)\n",
    "        hidden = self.dropout(hidden.squeeze(0))  # Apply dropout here\n",
    "        out = self.fc(hidden)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00275e2e-d97c-4533-b13a-e652b0290b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0, path='checkpoint.pt', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last improvement before stopping.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            path (str): File path to save the best model.\n",
    "            verbose (bool): If True, prints messages during training.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0  # Counts how many epochs have passed without improvement\n",
    "        self.best_score = None  # Tracks the best (lowest) validation loss seen so far\n",
    "        self.early_stop = False  # Flag to indicate if early stopping should trigger\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Call this method after each epoch with the current validation loss and model.\n",
    "        \"\"\"\n",
    "        score = -val_loss  # Since lower loss is better, we negate to treat higher as better\n",
    "\n",
    "        # First call: set the initial best score and save the model\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self._save_checkpoint(model)\n",
    "\n",
    "        # If no significant improvement\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1  # Increase counter since no improvement\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                # If patience is exceeded, trigger early stopping\n",
    "                self.early_stop = True\n",
    "\n",
    "        # If improved\n",
    "        else:\n",
    "            self.best_score = score  # Update best score\n",
    "            self._save_checkpoint(model)  # Save model checkpoint\n",
    "            self.counter = 0  # Reset counter\n",
    "\n",
    "    def _save_checkpoint(self, model):\n",
    "        \"\"\"\n",
    "        Saves model state_dict to disk if validation loss improves.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss improved. Saving model to {self.path}\")\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78d45e29-418b-45b9-9528-ace051f570e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2 = SentimentLSTM(\n",
    "    vocab_size= len(train_dataset.vocab),\n",
    "    embed_dim = 64, \n",
    "    hidden_dim= 256,\n",
    "    output_dim=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "31f2ef13-3dd3-4855-a972-f000a3bd7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params= model_v2.parameters(),\n",
    "                            lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89783038-645b-45b2-a329-8fc5cfc58110",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=7, min_delta=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c904219-e8c1-413b-af39-176225368426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf1d41e42be46e49aeeab99a5ca9ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0---\n",
      "\n",
      "Train Loss: 0.6940 | Train Accuracy: 0.5053\n",
      "Test Loss: 0.6932 |  Test Accuracy: 0.5082\n",
      "Validation loss improved. Saving model to checkpoint.pt\n",
      "Epoch: 1---\n",
      "\n",
      "Train Loss: 0.6905 | Train Accuracy: 0.5159\n",
      "Test Loss: 0.6902 |  Test Accuracy: 0.5238\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch: 2---\n",
      "\n",
      "Train Loss: 0.6752 | Train Accuracy: 0.5731\n",
      "Test Loss: 0.6944 |  Test Accuracy: 0.5082\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch: 3---\n",
      "\n",
      "Train Loss: 0.6190 | Train Accuracy: 0.6233\n",
      "Test Loss: 0.4390 |  Test Accuracy: 0.8093\n",
      "Validation loss improved. Saving model to checkpoint.pt\n",
      "Epoch: 4---\n",
      "\n",
      "Train Loss: 0.3330 | Train Accuracy: 0.8624\n",
      "Test Loss: 0.3163 |  Test Accuracy: 0.8690\n",
      "Validation loss improved. Saving model to checkpoint.pt\n",
      "Epoch: 5---\n",
      "\n",
      "Train Loss: 0.2222 | Train Accuracy: 0.9163\n",
      "Test Loss: 0.3214 |  Test Accuracy: 0.8708\n",
      "EarlyStopping counter: 1 out of 7\n",
      "Epoch: 6---\n",
      "\n",
      "Train Loss: 0.1478 | Train Accuracy: 0.9499\n",
      "Test Loss: 0.3679 |  Test Accuracy: 0.8652\n",
      "EarlyStopping counter: 2 out of 7\n",
      "Epoch: 7---\n",
      "\n",
      "Train Loss: 0.0908 | Train Accuracy: 0.9735\n",
      "Test Loss: 0.3966 |  Test Accuracy: 0.8638\n",
      "EarlyStopping counter: 3 out of 7\n",
      "Epoch: 8---\n",
      "\n",
      "Train Loss: 0.0545 | Train Accuracy: 0.9859\n",
      "Test Loss: 0.5776 |  Test Accuracy: 0.8672\n",
      "EarlyStopping counter: 4 out of 7\n",
      "Epoch: 9---\n",
      "\n",
      "Train Loss: 0.0469 | Train Accuracy: 0.9872\n",
      "Test Loss: 0.5870 |  Test Accuracy: 0.8672\n",
      "EarlyStopping counter: 5 out of 7\n",
      "Epoch: 10---\n",
      "\n",
      "Train Loss: 0.0295 | Train Accuracy: 0.9933\n",
      "Test Loss: 0.5941 |  Test Accuracy: 0.8632\n",
      "EarlyStopping counter: 6 out of 7\n",
      "Epoch: 11---\n",
      "\n",
      "Train Loss: 0.0270 | Train Accuracy: 0.9936\n",
      "Test Loss: 0.5612 |  Test Accuracy: 0.8632\n",
      "EarlyStopping counter: 7 out of 7\n",
      "Early stopping triggered\n",
      "Train time on cuda:0: 311.146 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "torch.manual_seed(42)\n",
    "train_time_start_gpu= timer()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}---\\n\")\n",
    "    train_step(\n",
    "        model= model_v2,\n",
    "        train_dataloader=train_loader,\n",
    "        optimizer = optimizer, \n",
    "        loss_fn=loss_fn, \n",
    "        )\n",
    "    val_loss = validate_step(model= model_v2,\n",
    "        test_dataloader=val_loader,\n",
    "        loss_fn=loss_fn, \n",
    "        )\n",
    "    early_stopping(val_loss, model_v2)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "train_time_end_gpu = timer()\n",
    "total_train_time_model_2 = print_train_time(\n",
    "    start = train_time_start_gpu,\n",
    "    end = train_time_end_gpu,\n",
    "    device=str(next(model_v2.parameters()).device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3d7296e-d762-4bfc-a68e-0f25f2c7806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b1e03-db1a-41ff-ba99-0e6df92e44bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
