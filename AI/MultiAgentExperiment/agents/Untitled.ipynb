{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eabd49e-ae2f-49b0-8703-c06ed9576c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-12 16:28:45,326 - camel.models.model_manager - ERROR - Error processing with model: <camel.models.deepseek_model.DeepSeekModel object at 0x0000014C1BAC7DF0>\n",
      "2025-04-12 16:28:45,327 - camel.agents.chat_agent - ERROR - An error occurred while running model deepseek-chat, index: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\agents\\chat_agent.py\", line 791, in _get_model_response\n",
      "    response = self.model_backend.run(\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\models\\model_manager.py\", line 226, in run\n",
      "    raise exc\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\models\\model_manager.py\", line 216, in run\n",
      "    response = self.current_model.run(messages, response_format, tools)\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\models\\base_model.py\", line 50, in wrapped_run\n",
      "    return original_run(self, messages, *args, **kwargs)\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\models\\base_model.py\", line 278, in run\n",
      "    return self._run(messages, response_format, tools)\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\camel\\models\\deepseek_model.py\", line 223, in _run\n",
      "    response = self._client.chat.completions.create(\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\openai\\_utils\\_utils.py\", line 279, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 914, in create\n",
      "    return self._post(\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\openai\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\openai\\_base_client.py\", line 919, in request\n",
      "    return self._request(\n",
      "  File \"C:\\learning-progress\\env\\lib\\site-packages\\openai\\_base_client.py\", line 1023, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.APIStatusError: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    },
    {
     "ename": "ModelProcessingError",
     "evalue": "Unable to process messages: the only provided model did not run successfully. Error: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelProcessingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m agent \u001b[38;5;241m=\u001b[39m ChatAgent(agent_sys_msg, model\u001b[38;5;241m=\u001b[39mollama_model, token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     17\u001b[0m user_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay hi to CAMEL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m assistant_response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(assistant_response\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mC:\\learning-progress\\env\\lib\\site-packages\\camel\\agents\\chat_agent.py:609\u001b[0m, in \u001b[0;36mChatAgent.step\u001b[1;34m(self, input_message, response_format)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_token_exceed(\n\u001b[0;32m    606\u001b[0m         e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m], tool_call_records, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens_exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    607\u001b[0m     )\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# Get response from model backend\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenai_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_full_tool_schemas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool_call_requests \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mtool_call_requests:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Process all tool calls\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tool_call_request \u001b[38;5;129;01min\u001b[39;00m tool_call_requests:\n",
      "File \u001b[1;32mC:\\learning-progress\\env\\lib\\site-packages\\camel\\agents\\chat_agent.py:809\u001b[0m, in \u001b[0;36mChatAgent._get_model_response\u001b[1;34m(self, openai_messages, num_tokens, response_format, tool_schemas)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[0;32m    805\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to process messages: none of the provided models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelProcessingError(\n\u001b[0;32m    810\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to process messages: the only provided model \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdid not run successfully. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m     )\n\u001b[0;32m    814\u001b[0m sanitized_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_messages_for_logging(\n\u001b[0;32m    815\u001b[0m     openai_messages\n\u001b[0;32m    816\u001b[0m )\n\u001b[0;32m    817\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_backend\u001b[38;5;241m.\u001b[39mcurrent_model_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed these messages: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    821\u001b[0m )\n",
      "\u001b[1;31mModelProcessingError\u001b[0m: Unable to process messages: the only provided model did not run successfully. Error: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType\n",
    "\n",
    "ollama_model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.DEEPSEEK,\n",
    "    model_type=\"deepseek-chat\",\n",
    "    api_key='',\n",
    "    model_config_dict={\"temperature\": 0.4},\n",
    ")\n",
    "\n",
    "agent_sys_msg = \"You are a helpful assistant.\"\n",
    "\n",
    "agent = ChatAgent(agent_sys_msg, model=ollama_model, token_limit=4096)\n",
    "\n",
    "user_msg = \"Say hi to CAMEL\"\n",
    "\n",
    "assistant_response = agent.step(user_msg)\n",
    "print(assistant_response.msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98243139-d6bd-4f75-bae1-d8a28123ab7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ollama' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ollama' has no attribute 'models'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91882b13-907c-44cf-b433-ed059d4b06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests_oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests_oauthlib)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\learning-progress\\env\\lib\\site-packages (from requests_oauthlib) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\learning-progress\\env\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\learning-progress\\env\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\learning-progress\\env\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\learning-progress\\env\\lib\\site-packages (from requests>=2.0.0->requests_oauthlib) (2025.1.31)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: oauthlib, requests_oauthlib\n",
      "Successfully installed oauthlib-3.2.2 requests_oauthlib-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests_oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44220f-ef68-457a-9869-b31c529b8c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
